{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PPO_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iAbhyuday/AI-playground/blob/master/PPO_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JowRQEGGKQ"
      },
      "source": [
        "################################################################################\n",
        "> # **Clone GitHub repository**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyGzuEMQF6sJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05aab4fe-0036-4e23-e799-2281c4dbc985"
      },
      "source": [
        "\n",
        "################# Clone repository from github to colab session ################\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to clone all the preTrained networks, logs, graph figures, gifs \n",
        "from the GitHub repository to this colab session\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!git clone https://github.com/nikhilbarhate99/PPO-PyTorch\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "Cloning into 'PPO-PyTorch'...\n",
            "remote: Enumerating objects: 347, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 347 (delta 41), reused 93 (delta 11), pack-reused 209\u001b[K\n",
            "Receiving objects: 100% (347/347), 12.35 MiB | 20.23 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mrn6rpJpF8Sc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3b441c-4bad-433c-c27f-759445269346"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to copy all files and folders from cloned folder (PPO-PyTorch)\n",
        "to current directory (/content/ or ./)\n",
        "\n",
        "So you can load preTrained networks and log files without changing any paths\n",
        "\n",
        "**  This will overwrite any saved networks, logs, graph figures, or gifs \n",
        "    that are created in this session before copying having the same name (or number)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "!cp -rv ./PPO-PyTorch/* ./\n",
        "\n",
        "print(\"============================================================================================\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "'./PPO-PyTorch/LICENSE' -> './LICENSE'\n",
            "'./PPO-PyTorch/make_gif.py' -> './make_gif.py'\n",
            "'./PPO-PyTorch/plot_graph.py' -> './plot_graph.py'\n",
            "'./PPO-PyTorch/PPO_colab.ipynb' -> './PPO_colab.ipynb'\n",
            "'./PPO-PyTorch/PPO_figs' -> './PPO_figs'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2' -> './PPO_figs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png' -> './PPO_figs/BipedalWalker-v2/PPO_BipedalWalker-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1' -> './PPO_figs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png' -> './PPO_figs/CartPole-v1/PPO_CartPole-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2' -> './PPO_figs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png' -> './PPO_figs/LunarLander-v2/PPO_LunarLander-v2_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1' -> './PPO_figs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png' -> './PPO_figs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1' -> './PPO_figs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png' -> './PPO_figs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1' -> './PPO_figs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_0.png'\n",
            "'./PPO-PyTorch/PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png' -> './PPO_figs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_fig_1.png'\n",
            "'./PPO-PyTorch/PPO_gifs' -> './PPO_gifs'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2' -> './PPO_gifs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif' -> './PPO_gifs/BipedalWalker-v2/PPO_BipedalWalker-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1' -> './PPO_gifs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif' -> './PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2' -> './PPO_gifs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif' -> './PPO_gifs/LunarLander-v2/PPO_LunarLander-v2_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1' -> './PPO_gifs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1' -> './PPO_gifs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif' -> './PPO_gifs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1' -> './PPO_gifs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif' -> './PPO_gifs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_gif_0.gif'\n",
            "'./PPO-PyTorch/PPO_logs' -> './PPO_logs'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2' -> './PPO_logs/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv' -> './PPO_logs/BipedalWalker-v2/PPO_BipedalWalker-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1' -> './PPO_logs/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv' -> './PPO_logs/CartPole-v1/PPO_CartPole-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2' -> './PPO_logs/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv' -> './PPO_logs/LunarLander-v2/PPO_LunarLander-v2_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1' -> './PPO_logs/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv' -> './PPO_logs/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1' -> './PPO_logs/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv' -> './PPO_logs/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1' -> './PPO_logs/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_0.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_1.csv'\n",
            "'./PPO-PyTorch/PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv' -> './PPO_logs/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_log_2.csv'\n",
            "'./PPO-PyTorch/PPO_preTrained' -> './PPO_preTrained'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2' -> './PPO_preTrained/BipedalWalker-v2'\n",
            "'./PPO-PyTorch/PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth' -> './PPO_preTrained/BipedalWalker-v2/PPO_BipedalWalker-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1' -> './PPO_preTrained/CartPole-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth' -> './PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2' -> './PPO_preTrained/LunarLander-v2'\n",
            "'./PPO-PyTorch/PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth' -> './PPO_preTrained/LunarLander-v2/PPO_LunarLander-v2_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/README.md' -> './PPO_preTrained/README.md'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1' -> './PPO_preTrained/RoboschoolHalfCheetah-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHalfCheetah-v1/PPO_RoboschoolHalfCheetah-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1' -> './PPO_preTrained/RoboschoolHopper-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth' -> './PPO_preTrained/RoboschoolHopper-v1/PPO_RoboschoolHopper-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1' -> './PPO_preTrained/RoboschoolWalker2d-v1'\n",
            "'./PPO-PyTorch/PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth' -> './PPO_preTrained/RoboschoolWalker2d-v1/PPO_RoboschoolWalker2d-v1_0_0.pth'\n",
            "'./PPO-PyTorch/PPO.py' -> './PPO.py'\n",
            "'./PPO-PyTorch/README.md' -> './README.md'\n",
            "'./PPO-PyTorch/test.py' -> './test.py'\n",
            "'./PPO-PyTorch/train.py' -> './train.py'\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-7AbGA2F8Ut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57d15daa-3efe-44d1-fc07-4b25492bd912"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "\n",
        "run this section if you want to delete original cloned folder and the cloned ipynb file\n",
        "(after you have copied its contents to current directory)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "# delete original cloned folder\n",
        "!rm -r ./PPO-PyTorch\n",
        "\n",
        "# delete cloned ipynb file\n",
        "!rm ./PPO_colab.ipynb\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4VJcUT2GlJz"
      },
      "source": [
        "################################################################################\n",
        "> # **Install Dependencies**\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbpSQTflGlAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "699a9027-77fc-4ca7-e81f-c29788c782a8"
      },
      "source": [
        "\n",
        "\n",
        "############ install compatible version of OpenAI roboschool and gym ###########\n",
        "\n",
        "!pip install roboschool==1.0.48 gym==0.15.4\n",
        "\n",
        "!pip install box2d-py\n",
        "\n",
        "!pip install pybullet\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting roboschool==1.0.48\n",
            "  Downloading roboschool-1.0.48-cp37-cp37m-manylinux1_x86_64.whl (44.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 44.9 MB 20 kB/s \n",
            "\u001b[?25hCollecting gym==0.15.4\n",
            "  Downloading gym-0.15.4.tar.gz (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (1.15.0)\n",
            "Collecting pyglet<=1.3.2,>=1.2.0\n",
            "  Downloading pyglet-1.3.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting cloudpickle~=1.2.0\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym==0.15.4) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.3.2,>=1.2.0->gym==0.15.4) (0.16.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.4-py3-none-any.whl size=1648485 sha256=082916a502b59ff831080d79efc5e494dac604e939d12d08ccf1c571a72954e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/97/51/3adbfe67f40bce89b8eba2d3b8f42ec1c9f9c1e6305a73510d\n",
            "Successfully built gym\n",
            "Installing collected packages: pyglet, cloudpickle, gym, roboschool\n",
            "  Attempting uninstall: pyglet\n",
            "    Found existing installation: pyglet 1.5.0\n",
            "    Uninstalling pyglet-1.5.0:\n",
            "      Successfully uninstalled pyglet-1.5.0\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.17.3\n",
            "    Uninstalling gym-0.17.3:\n",
            "      Successfully uninstalled gym-0.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-1.2.2 gym-0.15.4 pyglet-1.3.2 roboschool-1.0.48\n",
            "Collecting box2d-py\n",
            "  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 806 kB/s \n",
            "\u001b[?25hInstalling collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.8\n",
            "Collecting pybullet\n",
            "  Downloading pybullet-3.1.7.tar.gz (79.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.0 MB 19 kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-3.1.7-cp37-cp37m-linux_x86_64.whl size=89750998 sha256=02d3d4d71acff39e2110e536f9b4d8feebeae2c4b41215e024d233cfd915e655\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/1c/62/86c8b68885c24123d87c5392d6678aa2b68a1796c8113e1aa6\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-3.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzZairIiGQ11"
      },
      "source": [
        "################################################################################\n",
        "> # **Introduction**\n",
        "> The notebook is divided into 5 major parts : \n",
        "\n",
        "*   **Part I** : define actor-critic network and PPO algorithm\n",
        "*   **Part II** : train PPO algorithm and save network weights and log files\n",
        "*   **Part III** : load (preTrained) network weights and test PPO algorithm\n",
        "*   **Part IV** : load log files and plot graphs\n",
        "*   **Part V** : install xvbf, load (preTrained) network weights and save images for gif and then generate gif\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s37cJXAYGrTY"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - I**\n",
        "\n",
        "*   define actor critic networks\n",
        "*   define PPO algorithm\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT6VUBg-F8Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28761e7b-b464-4062-e965-ba2102dddf69"
      },
      "source": [
        "\n",
        "\n",
        "############################### Import libraries ###############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import pybullet_envs\n",
        "\n",
        "\n",
        "################################## set device ##################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# set device to cpu or cuda\n",
        "device = torch.device('cpu')\n",
        "\n",
        "if(torch.cuda.is_available()): \n",
        "    device = torch.device('cuda:0') \n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
        "else:\n",
        "    print(\"Device set to : cpu\")\n",
        "    \n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################## PPO Policy ##################################\n",
        "\n",
        "\n",
        "class RolloutBuffer:\n",
        "    def __init__(self):\n",
        "        self.actions = []\n",
        "        self.states = []\n",
        "        self.logprobs = []\n",
        "        self.rewards = []\n",
        "        self.is_terminals = []\n",
        "    \n",
        "\n",
        "    def clear(self):\n",
        "        del self.actions[:]\n",
        "        del self.states[:]\n",
        "        del self.logprobs[:]\n",
        "        del self.rewards[:]\n",
        "        del self.is_terminals[:]\n",
        "\n",
        "\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n",
        "        super(ActorCritic, self).__init__()\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_dim = action_dim\n",
        "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n",
        "\n",
        "        # actor\n",
        "        if has_continuous_action_space :\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Tanh()\n",
        "                        )\n",
        "        else:\n",
        "            self.actor = nn.Sequential(\n",
        "                            nn.Linear(state_dim, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, 64),\n",
        "                            nn.Tanh(),\n",
        "                            nn.Linear(64, action_dim),\n",
        "                            nn.Softmax(dim=-1)\n",
        "                        )\n",
        "\n",
        "        \n",
        "        # critic\n",
        "        self.critic = nn.Sequential(\n",
        "                        nn.Linear(state_dim, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 64),\n",
        "                        nn.Tanh(),\n",
        "                        nn.Linear(64, 1)\n",
        "                    )\n",
        "        \n",
        "    def set_action_std(self, new_action_std):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "    \n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action = dist.sample()\n",
        "        action_logprob = dist.log_prob(action)\n",
        "        \n",
        "        return action.detach(), action_logprob.detach()\n",
        "    \n",
        "\n",
        "    def evaluate(self, state, action):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            action_mean = self.actor(state)\n",
        "            action_var = self.action_var.expand_as(action_mean)\n",
        "            cov_mat = torch.diag_embed(action_var).to(device)\n",
        "            dist = MultivariateNormal(action_mean, cov_mat)\n",
        "            \n",
        "            # for single action continuous environments\n",
        "            if self.action_dim == 1:\n",
        "                action = action.reshape(-1, self.action_dim)\n",
        "\n",
        "        else:\n",
        "            action_probs = self.actor(state)\n",
        "            dist = Categorical(action_probs)\n",
        "\n",
        "        action_logprobs = dist.log_prob(action)\n",
        "        dist_entropy = dist.entropy()\n",
        "        state_values = self.critic(state)\n",
        "        \n",
        "        return action_logprobs, state_values, dist_entropy\n",
        "\n",
        "\n",
        "class PPO:\n",
        "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
        "\n",
        "        self.has_continuous_action_space = has_continuous_action_space\n",
        "\n",
        "        if has_continuous_action_space:\n",
        "            self.action_std = action_std_init\n",
        "\n",
        "        self.gamma = gamma\n",
        "        self.eps_clip = eps_clip\n",
        "        self.K_epochs = K_epochs\n",
        "        \n",
        "        self.buffer = RolloutBuffer()\n",
        "\n",
        "        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.optimizer = torch.optim.Adam([\n",
        "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
        "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
        "                    ])\n",
        "\n",
        "        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "        \n",
        "        self.MseLoss = nn.MSELoss()\n",
        "\n",
        "\n",
        "    def set_action_std(self, new_action_std):\n",
        "        \n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = new_action_std\n",
        "            self.policy.set_action_std(new_action_std)\n",
        "            self.policy_old.set_action_std(new_action_std)\n",
        "        \n",
        "        else:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            self.action_std = self.action_std - action_std_decay_rate\n",
        "            self.action_std = round(self.action_std, 4)\n",
        "            if (self.action_std <= min_action_std):\n",
        "                self.action_std = min_action_std\n",
        "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
        "            else:\n",
        "                print(\"setting actor output action_std to : \", self.action_std)\n",
        "            self.set_action_std(self.action_std)\n",
        "\n",
        "        else:\n",
        "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
        "\n",
        "        print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "\n",
        "        if self.has_continuous_action_space:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "\n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state = torch.FloatTensor(state).to(device)\n",
        "                action, action_logprob = self.policy_old.act(state)\n",
        "            \n",
        "            self.buffer.states.append(state)\n",
        "            self.buffer.actions.append(action)\n",
        "            self.buffer.logprobs.append(action_logprob)\n",
        "\n",
        "            return action.item()\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "\n",
        "        # Monte Carlo estimate of returns\n",
        "        rewards = []\n",
        "        discounted_reward = 0\n",
        "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
        "            if is_terminal:\n",
        "                discounted_reward = 0\n",
        "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
        "            rewards.insert(0, discounted_reward)\n",
        "            \n",
        "        # Normalizing the rewards\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
        "\n",
        "        # convert list to tensor\n",
        "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n",
        "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n",
        "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n",
        "\n",
        "        \n",
        "        # Optimize policy for K epochs\n",
        "        for _ in range(self.K_epochs):\n",
        "\n",
        "            # Evaluating old actions and values\n",
        "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
        "\n",
        "            # match state_values tensor dimensions with rewards tensor\n",
        "            state_values = torch.squeeze(state_values)\n",
        "            \n",
        "            # Finding the ratio (pi_theta / pi_theta__old)\n",
        "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
        "\n",
        "            # Finding Surrogate Loss\n",
        "            advantages = rewards - state_values.detach()   \n",
        "            surr1 = ratios * advantages\n",
        "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
        "\n",
        "            # final loss of clipped objective PPO\n",
        "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
        "            \n",
        "            # take gradient step\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.mean().backward()\n",
        "            self.optimizer.step()\n",
        "            \n",
        "        # Copy new weights into old policy\n",
        "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "        # clear buffer\n",
        "        self.buffer.clear()\n",
        "    \n",
        "    \n",
        "    def save(self, checkpoint_path):\n",
        "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
        "   \n",
        "\n",
        "    def load(self, checkpoint_path):\n",
        "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
        "        \n",
        "        \n",
        "       \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "Device set to : cpu\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xCb_EyxF8cF"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part I ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr-ZjT_CGyEi"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - II**\n",
        "\n",
        "*   train PPO algorithm on environments\n",
        "*   save preTrained networks weights and log files\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY1-DzVCF8eh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a3dad6c-b605-4147-eee4-01b56e2faa8c"
      },
      "source": [
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################################### Training ###################################\n",
        "\n",
        "\n",
        "####### initialize environment hyperparameters ######\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "\n",
        "max_ep_len = 400                    # max timesteps in one episode\n",
        "max_training_timesteps = int(1e5)   # break training loop if timeteps > max_training_timesteps\n",
        "\n",
        "print_freq = max_ep_len * 4     # print avg reward in the interval (in num timesteps)\n",
        "log_freq = max_ep_len * 2       # log avg reward in the interval (in num timesteps)\n",
        "save_model_freq = int(2e4)      # save model frequency (in num timesteps)\n",
        "\n",
        "action_std = None\n",
        "\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "## Note : print/log frequencies should be > than max_ep_len\n",
        "\n",
        "\n",
        "################ PPO hyperparameters ################\n",
        "\n",
        "\n",
        "update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
        "K_epochs = 40               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003       # learning rate for actor network\n",
        "lr_critic = 0.001       # learning rate for critic network\n",
        "\n",
        "random_seed = 0         # set random seed if required (0 = no random seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "\n",
        "print(\"training environment name : \" + env_name)\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "###################### logging ######################\n",
        "\n",
        "#### log files for multiple runs are NOT overwritten\n",
        "\n",
        "log_dir = \"PPO_logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "log_dir = log_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(log_dir):\n",
        "      os.makedirs(log_dir)\n",
        "\n",
        "\n",
        "#### get number of log files in log directory\n",
        "run_num = 0\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "run_num = len(current_num_files)\n",
        "\n",
        "\n",
        "#### create new log file for each run \n",
        "log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "\n",
        "print(\"current logging run number for \" + env_name + \" : \", run_num)\n",
        "print(\"logging at : \" + log_f_name)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "################### checkpointing ###################\n",
        "\n",
        "run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
        "\n",
        "directory = \"PPO_preTrained\"\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "directory = directory + '/' + env_name + '/'\n",
        "if not os.path.exists(directory):\n",
        "      os.makedirs(directory)\n",
        "\n",
        "\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"save checkpoint path : \" + checkpoint_path)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "############# print all hyperparameters #############\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"max training timesteps : \", max_training_timesteps)\n",
        "print(\"max timesteps per episode : \", max_ep_len)\n",
        "\n",
        "print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
        "print(\"log frequency : \" + str(log_freq) + \" timesteps\")\n",
        "print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"state space dimension : \", state_dim)\n",
        "print(\"action space dimension : \", action_dim)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "if has_continuous_action_space:\n",
        "    print(\"Initializing a continuous action space policy\")\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"starting std of action distribution : \", action_std)\n",
        "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
        "    print(\"minimum std of action distribution : \", min_action_std)\n",
        "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
        "\n",
        "else:\n",
        "    print(\"Initializing a discrete action space policy\")\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\") \n",
        "print(\"PPO K epochs : \", K_epochs)\n",
        "print(\"PPO epsilon clip : \", eps_clip)\n",
        "print(\"discount factor (gamma) : \", gamma)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(\"optimizer learning rate actor : \", lr_actor)\n",
        "print(\"optimizer learning rate critic : \", lr_critic)\n",
        "\n",
        "if random_seed:\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "    print(\"setting random seed to \", random_seed)\n",
        "    torch.manual_seed(random_seed)\n",
        "    env.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "\n",
        "#####################################################\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "################# training procedure ################\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# track total training time\n",
        "start_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "# logging file\n",
        "log_f = open(log_f_name,\"w+\")\n",
        "log_f.write('episode,timestep,reward\\n')\n",
        "\n",
        "\n",
        "# printing and logging variables\n",
        "print_running_reward = 0\n",
        "print_running_episodes = 0\n",
        "\n",
        "log_running_reward = 0\n",
        "log_running_episodes = 0\n",
        "\n",
        "time_step = 0\n",
        "i_episode = 0\n",
        "\n",
        "\n",
        "# training loop\n",
        "while time_step <= max_training_timesteps:\n",
        "    \n",
        "    state = env.reset()\n",
        "    current_ep_reward = 0\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        \n",
        "        # select action with policy\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # saving reward and is_terminals\n",
        "        ppo_agent.buffer.rewards.append(reward)\n",
        "        ppo_agent.buffer.is_terminals.append(done)\n",
        "        \n",
        "        time_step +=1\n",
        "        current_ep_reward += reward\n",
        "\n",
        "        # update PPO agent\n",
        "        if time_step % update_timestep == 0:\n",
        "            ppo_agent.update()\n",
        "\n",
        "        # if continuous action space; then decay action std of ouput action distribution\n",
        "        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n",
        "            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
        "\n",
        "        # log in logging file\n",
        "        if time_step % log_freq == 0:\n",
        "\n",
        "            # log average reward till last episode\n",
        "            log_avg_reward = log_running_reward / log_running_episodes\n",
        "            log_avg_reward = round(log_avg_reward, 4)\n",
        "\n",
        "            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "            log_f.flush()\n",
        "\n",
        "            log_running_reward = 0\n",
        "            log_running_episodes = 0\n",
        "\n",
        "        # printing average reward\n",
        "        if time_step % print_freq == 0:\n",
        "\n",
        "            # print average reward till last episode\n",
        "            print_avg_reward = print_running_reward / print_running_episodes\n",
        "            print_avg_reward = round(print_avg_reward, 2)\n",
        "\n",
        "            print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "            print_running_reward = 0\n",
        "            print_running_episodes = 0\n",
        "            \n",
        "        # save model weights\n",
        "        if time_step % save_model_freq == 0:\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            print(\"saving model at : \" + checkpoint_path)\n",
        "            ppo_agent.save(checkpoint_path)\n",
        "            print(\"model saved\")\n",
        "            print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n",
        "            print(\"--------------------------------------------------------------------------------------------\")\n",
        "            \n",
        "        # break; if the episode is over\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print_running_reward += current_ep_reward\n",
        "    print_running_episodes += 1\n",
        "\n",
        "    log_running_reward += current_ep_reward\n",
        "    log_running_episodes += 1\n",
        "\n",
        "    i_episode += 1\n",
        "\n",
        "\n",
        "log_f.close()\n",
        "env.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# print total training time\n",
        "print(\"============================================================================================\")\n",
        "end_time = datetime.now().replace(microsecond=0)\n",
        "print(\"Started training at (GMT) : \", start_time)\n",
        "print(\"Finished training at (GMT) : \", end_time)\n",
        "print(\"Total training time  : \", end_time - start_time)\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "training environment name : CartPole-v1\n",
            "current logging run number for CartPole-v1 :  0\n",
            "logging at : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "save checkpoint path : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "max training timesteps :  100000\n",
            "max timesteps per episode :  400\n",
            "model saving frequency : 20000 timesteps\n",
            "log frequency : 800 timesteps\n",
            "printing average reward over episodes in last : 1600 timesteps\n",
            "--------------------------------------------------------------------------------------------\n",
            "state space dimension :  4\n",
            "action space dimension :  2\n",
            "--------------------------------------------------------------------------------------------\n",
            "Initializing a discrete action space policy\n",
            "--------------------------------------------------------------------------------------------\n",
            "PPO update frequency : 1600 timesteps\n",
            "PPO K epochs :  40\n",
            "PPO epsilon clip :  0.2\n",
            "discount factor (gamma) :  0.99\n",
            "--------------------------------------------------------------------------------------------\n",
            "optimizer learning rate actor :  0.0003\n",
            "optimizer learning rate critic :  0.001\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2021-07-31 17:41:13\n",
            "============================================================================================\n",
            "Episode : 85 \t\t Timestep : 1600 \t\t Average Reward : 18.6\n",
            "Episode : 148 \t\t Timestep : 3200 \t\t Average Reward : 24.29\n",
            "Episode : 195 \t\t Timestep : 4800 \t\t Average Reward : 34.91\n",
            "Episode : 227 \t\t Timestep : 6400 \t\t Average Reward : 51.0\n",
            "Episode : 250 \t\t Timestep : 8000 \t\t Average Reward : 70.0\n",
            "Episode : 266 \t\t Timestep : 9600 \t\t Average Reward : 99.12\n",
            "Episode : 278 \t\t Timestep : 11200 \t\t Average Reward : 132.75\n",
            "Episode : 291 \t\t Timestep : 12800 \t\t Average Reward : 122.15\n",
            "Episode : 301 \t\t Timestep : 14400 \t\t Average Reward : 160.9\n",
            "Episode : 306 \t\t Timestep : 16000 \t\t Average Reward : 263.6\n",
            "Episode : 313 \t\t Timestep : 17600 \t\t Average Reward : 269.43\n",
            "Episode : 318 \t\t Timestep : 19200 \t\t Average Reward : 267.2\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:11\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 323 \t\t Timestep : 20800 \t\t Average Reward : 346.6\n",
            "Episode : 328 \t\t Timestep : 22400 \t\t Average Reward : 334.6\n",
            "Episode : 336 \t\t Timestep : 24000 \t\t Average Reward : 209.75\n",
            "Episode : 340 \t\t Timestep : 25600 \t\t Average Reward : 307.0\n",
            "Episode : 345 \t\t Timestep : 27200 \t\t Average Reward : 377.4\n",
            "Episode : 351 \t\t Timestep : 28800 \t\t Average Reward : 247.83\n",
            "Episode : 355 \t\t Timestep : 30400 \t\t Average Reward : 368.5\n",
            "Episode : 360 \t\t Timestep : 32000 \t\t Average Reward : 350.2\n",
            "Episode : 365 \t\t Timestep : 33600 \t\t Average Reward : 287.0\n",
            "Episode : 370 \t\t Timestep : 35200 \t\t Average Reward : 384.8\n",
            "Episode : 374 \t\t Timestep : 36800 \t\t Average Reward : 400.0\n",
            "Episode : 380 \t\t Timestep : 38400 \t\t Average Reward : 234.0\n",
            "Episode : 385 \t\t Timestep : 40000 \t\t Average Reward : 332.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:23\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 389 \t\t Timestep : 41600 \t\t Average Reward : 344.75\n",
            "Episode : 394 \t\t Timestep : 43200 \t\t Average Reward : 333.0\n",
            "Episode : 399 \t\t Timestep : 44800 \t\t Average Reward : 350.2\n",
            "Episode : 403 \t\t Timestep : 46400 \t\t Average Reward : 399.0\n",
            "Episode : 407 \t\t Timestep : 48000 \t\t Average Reward : 400.0\n",
            "Episode : 411 \t\t Timestep : 49600 \t\t Average Reward : 373.0\n",
            "Episode : 416 \t\t Timestep : 51200 \t\t Average Reward : 323.6\n",
            "Episode : 420 \t\t Timestep : 52800 \t\t Average Reward : 400.0\n",
            "Episode : 424 \t\t Timestep : 54400 \t\t Average Reward : 400.0\n",
            "Episode : 429 \t\t Timestep : 56000 \t\t Average Reward : 325.8\n",
            "Episode : 433 \t\t Timestep : 57600 \t\t Average Reward : 400.0\n",
            "Episode : 438 \t\t Timestep : 59200 \t\t Average Reward : 354.4\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:34\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 442 \t\t Timestep : 60800 \t\t Average Reward : 397.5\n",
            "Episode : 446 \t\t Timestep : 62400 \t\t Average Reward : 375.25\n",
            "Episode : 450 \t\t Timestep : 64000 \t\t Average Reward : 351.75\n",
            "Episode : 454 \t\t Timestep : 65600 \t\t Average Reward : 400.0\n",
            "Episode : 459 \t\t Timestep : 67200 \t\t Average Reward : 381.2\n",
            "Episode : 463 \t\t Timestep : 68800 \t\t Average Reward : 400.0\n",
            "Episode : 468 \t\t Timestep : 70400 \t\t Average Reward : 317.8\n",
            "Episode : 472 \t\t Timestep : 72000 \t\t Average Reward : 400.0\n",
            "Episode : 476 \t\t Timestep : 73600 \t\t Average Reward : 400.0\n",
            "Episode : 480 \t\t Timestep : 75200 \t\t Average Reward : 400.0\n",
            "Episode : 485 \t\t Timestep : 76800 \t\t Average Reward : 304.6\n",
            "Episode : 490 \t\t Timestep : 78400 \t\t Average Reward : 330.6\n",
            "Episode : 494 \t\t Timestep : 80000 \t\t Average Reward : 384.0\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:46\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode : 499 \t\t Timestep : 81600 \t\t Average Reward : 346.2\n",
            "Episode : 503 \t\t Timestep : 83200 \t\t Average Reward : 400.0\n",
            "Episode : 507 \t\t Timestep : 84800 \t\t Average Reward : 389.75\n",
            "Episode : 511 \t\t Timestep : 86400 \t\t Average Reward : 400.0\n",
            "Episode : 515 \t\t Timestep : 88000 \t\t Average Reward : 397.5\n",
            "Episode : 519 \t\t Timestep : 89600 \t\t Average Reward : 391.5\n",
            "Episode : 523 \t\t Timestep : 91200 \t\t Average Reward : 400.0\n",
            "Episode : 527 \t\t Timestep : 92800 \t\t Average Reward : 400.0\n",
            "Episode : 531 \t\t Timestep : 94400 \t\t Average Reward : 400.0\n",
            "Episode : 535 \t\t Timestep : 96000 \t\t Average Reward : 352.0\n",
            "Episode : 539 \t\t Timestep : 97600 \t\t Average Reward : 400.0\n",
            "Episode : 543 \t\t Timestep : 99200 \t\t Average Reward : 396.25\n",
            "--------------------------------------------------------------------------------------------\n",
            "saving model at : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "model saved\n",
            "Elapsed Time  :  0:00:57\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "Started training at (GMT) :  2021-07-31 17:41:13\n",
            "Finished training at (GMT) :  2021-07-31 17:42:10\n",
            "Total training time  :  0:00:57\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEy2qKdZF8ha"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part II ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhK13_1G6zX"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - III**\n",
        "\n",
        "*   load and test preTrained networks on environments\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZWyhkq9Gxm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c4d6ade-29df-4e9c-df02-747f26fa20bd"
      },
      "source": [
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "#################################### Testing ###################################\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 10    # total num of testing episodes\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003           # learning rate for actor\n",
        "lr_critic = 0.001           # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "# initialize a PPO agent\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "    \n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "\n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 400.0\n",
            "Episode: 2 \t\t Reward: 400.0\n",
            "Episode: 3 \t\t Reward: 400.0\n",
            "Episode: 4 \t\t Reward: 400.0\n",
            "Episode: 5 \t\t Reward: 400.0\n",
            "Episode: 6 \t\t Reward: 400.0\n",
            "Episode: 7 \t\t Reward: 400.0\n",
            "Episode: 8 \t\t Reward: 400.0\n",
            "Episode: 9 \t\t Reward: 400.0\n",
            "Episode: 10 \t\t Reward: 400.0\n",
            "============================================================================================\n",
            "average test reward : 400.0\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6IYC_JCGxlB"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part III ###############################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZewQELovHFt4"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - IV**\n",
        "\n",
        "*   load log files using pandas\n",
        "*   plot graph using matplotlib\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY-E5HGcGxiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "46ef1222-729e-4176-c746-fc61b16caeff"
      },
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "fig_num = 0     #### change this to prevent overwriting figures in same env_name folder\n",
        "\n",
        "plot_avg = True    # plot average of all runs; else plot all runs separately\n",
        "\n",
        "fig_width = 10\n",
        "fig_height = 6\n",
        "\n",
        "\n",
        "# smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "window_len_smooth = 50\n",
        "min_window_len_smooth = 1\n",
        "linewidth_smooth = 1.5\n",
        "alpha_smooth = 1\n",
        "\n",
        "window_len_var = 5\n",
        "min_window_len_var = 1\n",
        "linewidth_var = 2\n",
        "alpha_var = 0.1\n",
        "\n",
        "\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple', 'olive', 'brown', 'magenta', 'cyan', 'crimson','gray', 'black']\n",
        "\n",
        "\n",
        "# make directory for saving figures\n",
        "figures_dir = \"PPO_figs\"\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "# make environment directory for saving figures\n",
        "figures_dir = figures_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(figures_dir):\n",
        "    os.makedirs(figures_dir)\n",
        "\n",
        "\n",
        "fig_save_path = figures_dir + '/PPO_' + env_name + '_fig_' + str(fig_num) + '.png'\n",
        "\n",
        "\n",
        "# get number of log files in directory\n",
        "log_dir = \"PPO_logs\" + '/' + env_name + '/'\n",
        "\n",
        "current_num_files = next(os.walk(log_dir))[2]\n",
        "num_runs = len(current_num_files)\n",
        "\n",
        "\n",
        "all_runs = []\n",
        "\n",
        "for run_num in range(num_runs):\n",
        "\n",
        "    log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n",
        "    print(\"loading data from : \" + log_f_name)\n",
        "    data = pd.read_csv(log_f_name)\n",
        "    data = pd.DataFrame(data)\n",
        "    \n",
        "    print(\"data shape : \", data.shape)\n",
        "    \n",
        "    all_runs.append(data)\n",
        "    print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "if plot_avg:\n",
        "    # average all runs\n",
        "    df_concat = pd.concat(all_runs)\n",
        "    df_concat_groupby = df_concat.groupby(df_concat.index)\n",
        "    data_avg = df_concat_groupby.mean()\n",
        "\n",
        "    # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "    data_avg['reward_smooth'] = data_avg['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "    data_avg['reward_var'] = data_avg['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_smooth',ax=ax,color=colors[0],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "    data_avg.plot(kind='line', x='timestep' , y='reward_var',ax=ax,color=colors[0],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep only reward_smooth in the legend and rename it\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    ax.legend([handles[0]], [\"reward_avg_\" + str(len(all_runs)) + \"_runs\"], loc=2)\n",
        "\n",
        "\n",
        "else:\n",
        "    for i, run in enumerate(all_runs):\n",
        "        # smooth out rewards to get a smooth and a less smooth (var) plot lines\n",
        "        run['reward_smooth_' + str(i)] = run['reward'].rolling(window=window_len_smooth, win_type='triang', min_periods=min_window_len_smooth).mean()\n",
        "        run['reward_var_' + str(i)] = run['reward'].rolling(window=window_len_var, win_type='triang', min_periods=min_window_len_var).mean()\n",
        "        \n",
        "        # plot the lines\n",
        "        run.plot(kind='line', x='timestep' , y='reward_smooth_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_smooth, alpha=alpha_smooth)\n",
        "        run.plot(kind='line', x='timestep' , y='reward_var_' + str(i),ax=ax,color=colors[i % len(colors)],  linewidth=linewidth_var, alpha=alpha_var)\n",
        "\n",
        "    # keep alternate elements (reward_smooth_i) in the legend\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = []\n",
        "    new_labels = []\n",
        "    for i in range(len(handles)):\n",
        "        if(i%2 == 0):\n",
        "            new_handles.append(handles[i])\n",
        "            new_labels.append(labels[i])\n",
        "    ax.legend(new_handles, new_labels, loc=2)\n",
        "\n",
        "\n",
        "\n",
        "# ax.set_yticks(np.arange(0, 1800, 200))\n",
        "# ax.set_xticks(np.arange(0, int(4e6), int(5e5)))\n",
        "\n",
        "\n",
        "ax.grid(color='gray', linestyle='-', linewidth=1, alpha=0.2)\n",
        "\n",
        "ax.set_xlabel(\"Timesteps\", fontsize=12)\n",
        "ax.set_ylabel(\"Rewards\", fontsize=12)\n",
        "\n",
        "plt.title(env_name, fontsize=14)\n",
        "\n",
        "\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(fig_width, fig_height)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "plt.savefig(fig_save_path)\n",
        "print(\"figure saved at : \", fig_save_path)\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "loading data from : PPO_logs/CartPole-v1//PPO_CartPole-v1_log_0.csv\n",
            "data shape :  (125, 3)\n",
            "--------------------------------------------------------------------------------------------\n",
            "============================================================================================\n",
            "figure saved at :  PPO_figs/CartPole-v1//PPO_CartPole-v1_fig_0.png\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaWPRW9EGxgH"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################ End of Part IV ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8uG43MtHNGC"
      },
      "source": [
        "################################################################################\n",
        "> # **Part - V**\n",
        "\n",
        "*   install virtual display libraries for rendering on colab / remote server ^\n",
        "*   load preTrained networks and save images for gif\n",
        "*   generate and save gif from previously saved images\n",
        "\n",
        "*   ^ If running locally; do not install xvbf and pyvirtualdisplay. Just comment out the virtual display code and render it normally. \n",
        "*   ^ You will still require to use ipythondisplay, if you want to render it in the Jupyter Notebook.\n",
        "\n",
        "################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL3tpKf3HLAq"
      },
      "source": [
        "\n",
        "\n",
        "#### to render on colab / server / headless machine install virtual display libraries\n",
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Rx_IFKHK-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d91a010a-1684-455d-abb4-4dc91e637d8d"
      },
      "source": [
        "\n",
        "\n",
        "############################# save images for gif ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import gym\n",
        "import roboschool\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "One frame corresponding to each timestep is saved in a folder :\n",
        "\n",
        "PPO_gif_images/env_name/000001.jpg\n",
        "PPO_gif_images/env_name/000002.jpg\n",
        "PPO_gif_images/env_name/000003.jpg\n",
        "...\n",
        "...\n",
        "...\n",
        "\n",
        "\n",
        "if this section is run multiple times or for multiple episodes for the same env_name; \n",
        "then the saved images will be overwritten.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### beginning of virtual display code section\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "#### end of virtual display code section\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "################## hyperparameters ##################\n",
        "\n",
        "env_name = \"CartPole-v1\"\n",
        "has_continuous_action_space = False\n",
        "max_ep_len = 400\n",
        "action_std = None\n",
        "\n",
        "\n",
        "# env_name = \"LunarLander-v2\"\n",
        "# has_continuous_action_space = False\n",
        "# max_ep_len = 300\n",
        "# action_std = None\n",
        "\n",
        "# env_name = \"BipedalWalker-v2\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1500           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "# env_name = \"RoboschoolWalker2d-v1\"\n",
        "# has_continuous_action_space = True\n",
        "# max_ep_len = 1000           # max timesteps in one episode\n",
        "# action_std = 0.1            # set same std for action distribution which was used while saving\n",
        "\n",
        "\n",
        "total_test_episodes = 1     # save gif for only one episode\n",
        "\n",
        "render_ipython = False      # plot the images using matplotlib and ipythondisplay before saving (slow)\n",
        "\n",
        "K_epochs = 80               # update policy for K epochs\n",
        "eps_clip = 0.2              # clip parameter for PPO\n",
        "gamma = 0.99                # discount factor\n",
        "\n",
        "lr_actor = 0.0003         # learning rate for actor\n",
        "lr_critic = 0.001         # learning rate for critic\n",
        "\n",
        "#####################################################\n",
        "\n",
        "\n",
        "env = gym.make(env_name)\n",
        "\n",
        "# state space dimension\n",
        "state_dim = env.observation_space.shape[0]\n",
        "\n",
        "# action space dimension\n",
        "if has_continuous_action_space:\n",
        "    action_dim = env.action_space.shape[0]\n",
        "else:\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "\n",
        "\n",
        "# make directory for saving gif images\n",
        "gif_images_dir = \"PPO_gif_images\" + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make environment directory for saving gif images\n",
        "gif_images_dir = gif_images_dir + '/' + env_name + '/'\n",
        "if not os.path.exists(gif_images_dir):\n",
        "    os.makedirs(gif_images_dir)\n",
        "\n",
        "# make directory for gif\n",
        "gif_dir = \"PPO_gifs\" + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "# make environment directory for gif\n",
        "gif_dir = gif_dir + '/' + env_name  + '/'\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "\n",
        "\n",
        "ppo_agent = PPO(state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
        "\n",
        "\n",
        "# preTrained weights directory\n",
        "\n",
        "random_seed = 0             #### set this to load a particular checkpoint trained on random seed\n",
        "run_num_pretrained = 0      #### set this to load a particular checkpoint num\n",
        "\n",
        "\n",
        "directory = \"PPO_preTrained\" + '/' + env_name + '/'\n",
        "checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n",
        "print(\"loading network from : \" + checkpoint_path)\n",
        "\n",
        "ppo_agent.load(checkpoint_path)\n",
        "\n",
        "print(\"--------------------------------------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "\n",
        "test_running_reward = 0\n",
        "\n",
        "for ep in range(1, total_test_episodes+1):\n",
        "    \n",
        "    ep_reward = 0\n",
        "    state = env.reset()\n",
        "\n",
        "    for t in range(1, max_ep_len+1):\n",
        "        action = ppo_agent.select_action(state)\n",
        "        state, reward, done, _ = env.step(action)\n",
        "        ep_reward += reward\n",
        "\n",
        "        img = env.render(mode = 'rgb_array')\n",
        "\n",
        "\n",
        "        #### beginning of ipythondisplay code section 1\n",
        "\n",
        "        if render_ipython:\n",
        "            plt.imshow(img)\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        #### end of ipythondisplay code section 1\n",
        "\n",
        "\n",
        "        img = Image.fromarray(img)\n",
        "        img.save(gif_images_dir + '/' + str(t).zfill(6) + '.jpg')\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # clear buffer    \n",
        "    ppo_agent.buffer.clear()\n",
        "    \n",
        "    test_running_reward +=  ep_reward\n",
        "    print('Episode: {} \\t\\t Reward: {}'.format(ep, round(ep_reward, 2)))\n",
        "    ep_reward = 0\n",
        "\n",
        "\n",
        "\n",
        "env.close()\n",
        "\n",
        "\n",
        "#### beginning of ipythondisplay code section 2\n",
        "\n",
        "if render_ipython:\n",
        "    ipythondisplay.clear_output(wait=True)\n",
        "\n",
        "#### end of ipythondisplay code section 2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "print(\"total number of frames / timesteps / images saved : \", t)\n",
        "\n",
        "avg_test_reward = test_running_reward / total_test_episodes\n",
        "avg_test_reward = round(avg_test_reward, 2)\n",
        "print(\"average test reward : \" + str(avg_test_reward))\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "loading network from : PPO_preTrained/CartPole-v1/PPO_CartPole-v1_0_0.pth\n",
            "--------------------------------------------------------------------------------------------\n",
            "Episode: 1 \t\t Reward: 400.0\n",
            "============================================================================================\n",
            "total number of frames / timesteps / images saved :  400\n",
            "average test reward : 400.0\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoVshl_ZHK7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "169f66c6-4b56-4e90-c92d-1480f6121042"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "######################## generate gif from saved images ########################\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_num = 0     #### change this to prevent overwriting gifs in same env_name folder\n",
        "\n",
        "# adjust following parameters to get desired duration, size (bytes) and smoothness of gif\n",
        "total_timesteps = 300\n",
        "step = 10\n",
        "frame_duration = 150\n",
        "\n",
        "\n",
        "# input images\n",
        "gif_images_dir = \"PPO_gif_images/\" + env_name + '/*.jpg'\n",
        "\n",
        "\n",
        "# ouput gif path\n",
        "gif_dir = \"PPO_gifs\"\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_dir = gif_dir + '/' + env_name\n",
        "if not os.path.exists(gif_dir):\n",
        "    os.makedirs(gif_dir)\n",
        "\n",
        "gif_path = gif_dir + '/PPO_' + env_name + '_gif_' + str(gif_num) + '.gif'\n",
        "\n",
        "\n",
        "\n",
        "img_paths = sorted(glob.glob(gif_images_dir))\n",
        "img_paths = img_paths[:total_timesteps]\n",
        "img_paths = img_paths[::step]\n",
        "\n",
        "\n",
        "print(\"total frames in gif : \", len(img_paths))\n",
        "print(\"total duration of gif : \" + str(round(len(img_paths) * frame_duration / 1000, 2)) + \" seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# save gif\n",
        "img, *imgs = [Image.open(f) for f in img_paths]\n",
        "img.save(fp=gif_path, format='GIF', append_images=imgs, save_all=True, optimize=True, duration=frame_duration, loop=0)\n",
        "\n",
        "print(\"saved gif at : \", gif_path)\n",
        "\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================================================\n",
            "total frames in gif :  30\n",
            "total duration of gif : 4.5 seconds\n",
            "saved gif at :  PPO_gifs/CartPole-v1/PPO_CartPole-v1_gif_0.gif\n",
            "============================================================================================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20d1bR8xHK5j"
      },
      "source": [
        "\n",
        "############################# check gif byte size ##############################\n",
        "\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "# env_name = 'LunarLander-v2'\n",
        "# env_name = 'BipedalWalker-v2'\n",
        "# env_name = 'RoboschoolWalker2d-v1'\n",
        "\n",
        "\n",
        "gif_dir = \"PPO_gifs/\" + env_name + '/*.gif'\n",
        "\n",
        "gif_paths = sorted(glob.glob(gif_dir))\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    file_size = os.path.getsize(gif_path)\n",
        "    print(gif_path + '\\t\\t' + str(round(file_size / (1024 * 1024), 2)) + \" MB\")\n",
        "\n",
        "\n",
        "print(\"============================================================================================\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rM5UIAkcGxeA"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "################################# End of Part V ################################\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YUzQOu1HYHR"
      },
      "source": [
        "################################################################################\n",
        "\n",
        "---------------------------------------------------------------------------- That's all folks ! ----------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "################################################################################"
      ]
    }
  ]
}